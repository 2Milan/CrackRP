//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: UNKNOWN
// Unknown Toolkit Version
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_75, texmode_independent
.address_size 64

	// .globl	l_markov

.entry l_markov(
	.param .u64 .ptr .global .align 4 l_markov_param_0,
	.param .u64 .ptr .global .align 4 l_markov_param_1,
	.param .u64 .ptr .global .align 4 l_markov_param_2,
	.param .u64 l_markov_param_3,
	.param .u32 l_markov_param_4,
	.param .u32 l_markov_param_5,
	.param .u32 l_markov_param_6,
	.param .u32 l_markov_param_7,
	.param .u32 l_markov_param_8,
	.param .u64 l_markov_param_9
)
{
	.local .align 4 .b8 	__local_depot0[260];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .b32 	%r<131>;
	.reg .b64 	%rd<129>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd57, [l_markov_param_0];
	ld.param.u64 	%rd58, [l_markov_param_1];
	ld.param.u64 	%rd59, [l_markov_param_2];
	ld.param.u64 	%rd60, [l_markov_param_3];
	ld.param.u32 	%r24, [l_markov_param_4];
	ld.param.u32 	%r129, [l_markov_param_5];
	ld.param.u32 	%r26, [l_markov_param_6];
	ld.param.u32 	%r27, [l_markov_param_7];
	ld.param.u32 	%r28, [l_markov_param_8];
	ld.param.u64 	%rd61, [l_markov_param_9];
	add.u64 	%rd127, %SPL, 0;
	mov.u32 	%r29, %ctaid.x;
	mov.u32 	%r30, %ntid.x;
	mov.u32 	%r31, %tid.x;
	mov.b32 	%r32, %envreg3;
	add.s32 	%r33, %r31, %r32;
	mad.lo.s32 	%r34, %r30, %r29, %r33;
	cvt.s64.s32 	%rd2, %r34;
	setp.ge.u64 	%p1, %rd2, %rd61;
	@%p1 bra 	$L__BB0_30;

	mov.u32 	%r35, 0;
	st.local.u32 	[%rd127], %r35;
	st.local.u32 	[%rd127+4], %r35;
	st.local.u32 	[%rd127+8], %r35;
	st.local.u32 	[%rd127+12], %r35;
	st.local.u32 	[%rd127+16], %r35;
	st.local.u32 	[%rd127+20], %r35;
	st.local.u32 	[%rd127+24], %r35;
	st.local.u32 	[%rd127+28], %r35;
	st.local.u32 	[%rd127+32], %r35;
	st.local.u32 	[%rd127+36], %r35;
	st.local.u32 	[%rd127+40], %r35;
	st.local.u32 	[%rd127+44], %r35;
	st.local.u32 	[%rd127+48], %r35;
	st.local.u32 	[%rd127+52], %r35;
	st.local.u32 	[%rd127+56], %r35;
	st.local.u32 	[%rd127+60], %r35;
	st.local.u32 	[%rd127+64], %r35;
	st.local.u32 	[%rd127+68], %r35;
	st.local.u32 	[%rd127+72], %r35;
	st.local.u32 	[%rd127+76], %r35;
	st.local.u32 	[%rd127+80], %r35;
	st.local.u32 	[%rd127+84], %r35;
	st.local.u32 	[%rd127+88], %r35;
	st.local.u32 	[%rd127+92], %r35;
	st.local.u32 	[%rd127+96], %r35;
	st.local.u32 	[%rd127+100], %r35;
	st.local.u32 	[%rd127+104], %r35;
	st.local.u32 	[%rd127+108], %r35;
	st.local.u32 	[%rd127+112], %r35;
	st.local.u32 	[%rd127+116], %r35;
	st.local.u32 	[%rd127+120], %r35;
	st.local.u32 	[%rd127+124], %r35;
	st.local.u32 	[%rd127+128], %r35;
	st.local.u32 	[%rd127+132], %r35;
	st.local.u32 	[%rd127+136], %r35;
	st.local.u32 	[%rd127+140], %r35;
	st.local.u32 	[%rd127+144], %r35;
	st.local.u32 	[%rd127+148], %r35;
	st.local.u32 	[%rd127+152], %r35;
	st.local.u32 	[%rd127+156], %r35;
	st.local.u32 	[%rd127+160], %r35;
	st.local.u32 	[%rd127+164], %r35;
	st.local.u32 	[%rd127+168], %r35;
	st.local.u32 	[%rd127+172], %r35;
	st.local.u32 	[%rd127+176], %r35;
	st.local.u32 	[%rd127+180], %r35;
	st.local.u32 	[%rd127+184], %r35;
	st.local.u32 	[%rd127+188], %r35;
	st.local.u32 	[%rd127+192], %r35;
	st.local.u32 	[%rd127+196], %r35;
	st.local.u32 	[%rd127+200], %r35;
	st.local.u32 	[%rd127+204], %r35;
	st.local.u32 	[%rd127+208], %r35;
	st.local.u32 	[%rd127+212], %r35;
	st.local.u32 	[%rd127+216], %r35;
	st.local.u32 	[%rd127+220], %r35;
	st.local.u32 	[%rd127+224], %r35;
	st.local.u32 	[%rd127+228], %r35;
	st.local.u32 	[%rd127+232], %r35;
	st.local.u32 	[%rd127+236], %r35;
	st.local.u32 	[%rd127+240], %r35;
	st.local.u32 	[%rd127+244], %r35;
	st.local.u32 	[%rd127+248], %r35;
	st.local.u32 	[%rd127+252], %r35;
	add.s32 	%r1, %r129, %r24;
	st.local.u32 	[%rd127+256], %r1;
	add.s64 	%rd112, %rd2, %rd60;
	setp.eq.s32 	%p2, %r24, 0;
	@%p2 bra 	$L__BB0_23;

	mul.wide.u32 	%rd63, %r129, 1028;
	add.s64 	%rd121, %rd58, %rd63;
	add.s32 	%r37, %r24, -1;
	and.b32  	%r128, %r24, 3;
	setp.lt.u32 	%p3, %r37, 3;
	@%p3 bra 	$L__BB0_17;

	sub.s32 	%r123, %r24, %r128;
	shl.b32 	%r38, %r129, 3;
	and.b32  	%r39, %r38, 24;
	xor.b32  	%r4, %r39, 24;
	add.s32 	%r40, %r38, 8;
	not.b32 	%r41, %r40;
	and.b32  	%r5, %r41, 24;
	xor.b32  	%r6, %r39, 8;
	add.s32 	%r42, %r38, -8;
	not.b32 	%r43, %r42;
	and.b32  	%r7, %r43, 24;

$L__BB0_4:
	ld.global.u32 	%rd8, [%rd121+1024];
	and.b64  	%rd64, %rd112, -4294967296;
	setp.eq.s64 	%p4, %rd64, 0;
	@%p4 bra 	$L__BB0_6;

	div.u64 	%rd113, %rd112, %rd8;
	mul.lo.s64 	%rd65, %rd113, %rd8;
	sub.s64 	%rd114, %rd112, %rd65;
	bra.uni 	$L__BB0_7;

$L__BB0_6:
	cvt.u32.u64 	%r44, %rd8;
	cvt.u32.u64 	%r45, %rd112;
	div.u32 	%r46, %r45, %r44;
	mul.lo.s32 	%r47, %r46, %r44;
	sub.s32 	%r48, %r45, %r47;
	cvt.u64.u32 	%rd113, %r46;
	cvt.u64.u32 	%rd114, %r48;

$L__BB0_7:
	shl.b64 	%rd66, %rd114, 2;
	add.s64 	%rd67, %rd121, %rd66;
	ld.global.u32 	%r49, [%rd67];
	shl.b32 	%r50, %r49, %r4;
	and.b32  	%r51, %r129, -4;
	cvt.u64.u32 	%rd68, %r51;
	add.s64 	%rd69, %rd127, %rd68;
	ld.local.u32 	%r52, [%rd69];
	or.b32  	%r53, %r52, %r50;
	st.local.u32 	[%rd69], %r53;
	shl.b32 	%r54, %r129, 8;
	add.s32 	%r55, %r49, %r54;
	cvt.u64.u32 	%rd15, %r55;
	mul.wide.u32 	%rd70, %r55, 1028;
	add.s64 	%rd71, %rd59, %rd70;
	ld.global.u32 	%rd16, [%rd71+1024];
	and.b64  	%rd72, %rd113, -4294967296;
	setp.eq.s64 	%p5, %rd72, 0;
	@%p5 bra 	$L__BB0_9;

	div.u64 	%rd115, %rd113, %rd16;
	mul.lo.s64 	%rd73, %rd115, %rd16;
	sub.s64 	%rd116, %rd113, %rd73;
	bra.uni 	$L__BB0_10;

$L__BB0_9:
	cvt.u32.u64 	%r56, %rd16;
	cvt.u32.u64 	%r57, %rd113;
	div.u32 	%r58, %r57, %r56;
	mul.lo.s32 	%r59, %r58, %r56;
	sub.s32 	%r60, %r57, %r59;
	cvt.u64.u32 	%rd115, %r58;
	cvt.u64.u32 	%rd116, %r60;

$L__BB0_10:
	add.s32 	%r61, %r129, 1;
	mul.lo.s64 	%rd74, %rd15, 1028;
	add.s64 	%rd75, %rd59, %rd74;
	shl.b64 	%rd76, %rd116, 2;
	add.s64 	%rd77, %rd75, %rd76;
	ld.global.u32 	%r62, [%rd77];
	shl.b32 	%r63, %r62, %r5;
	and.b32  	%r64, %r61, -4;
	cvt.u64.u32 	%rd78, %r64;
	add.s64 	%rd79, %rd127, %rd78;
	ld.local.u32 	%r65, [%rd79];
	or.b32  	%r66, %r65, %r63;
	st.local.u32 	[%rd79], %r66;
	shl.b32 	%r67, %r61, 8;
	add.s32 	%r68, %r62, %r67;
	cvt.u64.u32 	%rd23, %r68;
	mul.wide.u32 	%rd80, %r68, 1028;
	add.s64 	%rd81, %rd59, %rd80;
	ld.global.u32 	%rd24, [%rd81+1024];
	and.b64  	%rd82, %rd115, -4294967296;
	setp.eq.s64 	%p6, %rd82, 0;
	@%p6 bra 	$L__BB0_12;

	div.u64 	%rd117, %rd115, %rd24;
	mul.lo.s64 	%rd83, %rd117, %rd24;
	sub.s64 	%rd118, %rd115, %rd83;
	bra.uni 	$L__BB0_13;

$L__BB0_12:
	cvt.u32.u64 	%r69, %rd24;
	cvt.u32.u64 	%r70, %rd115;
	div.u32 	%r71, %r70, %r69;
	mul.lo.s32 	%r72, %r71, %r69;
	sub.s32 	%r73, %r70, %r72;
	cvt.u64.u32 	%rd117, %r71;
	cvt.u64.u32 	%rd118, %r73;

$L__BB0_13:
	add.s32 	%r74, %r129, 2;
	mul.lo.s64 	%rd84, %rd23, 1028;
	add.s64 	%rd85, %rd59, %rd84;
	shl.b64 	%rd86, %rd118, 2;
	add.s64 	%rd87, %rd85, %rd86;
	ld.global.u32 	%r75, [%rd87];
	shl.b32 	%r76, %r75, %r6;
	and.b32  	%r77, %r74, -4;
	cvt.u64.u32 	%rd88, %r77;
	add.s64 	%rd89, %rd127, %rd88;
	ld.local.u32 	%r78, [%rd89];
	or.b32  	%r79, %r78, %r76;
	st.local.u32 	[%rd89], %r79;
	shl.b32 	%r80, %r74, 8;
	add.s32 	%r81, %r75, %r80;
	cvt.u64.u32 	%rd31, %r81;
	mul.wide.u32 	%rd90, %r81, 1028;
	add.s64 	%rd91, %rd59, %rd90;
	ld.global.u32 	%rd32, [%rd91+1024];
	and.b64  	%rd92, %rd117, -4294967296;
	setp.eq.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB0_15;

	div.u64 	%rd112, %rd117, %rd32;
	mul.lo.s64 	%rd93, %rd112, %rd32;
	sub.s64 	%rd120, %rd117, %rd93;
	bra.uni 	$L__BB0_16;

$L__BB0_15:
	cvt.u32.u64 	%r82, %rd32;
	cvt.u32.u64 	%r83, %rd117;
	div.u32 	%r84, %r83, %r82;
	mul.lo.s32 	%r85, %r84, %r82;
	sub.s32 	%r86, %r83, %r85;
	cvt.u64.u32 	%rd112, %r84;
	cvt.u64.u32 	%rd120, %r86;

$L__BB0_16:
	add.s32 	%r87, %r129, 3;
	mul.lo.s64 	%rd94, %rd31, 1028;
	add.s64 	%rd95, %rd59, %rd94;
	shl.b64 	%rd96, %rd120, 2;
	add.s64 	%rd97, %rd95, %rd96;
	ld.global.u32 	%r88, [%rd97];
	shl.b32 	%r89, %r88, %r7;
	and.b32  	%r90, %r87, -4;
	cvt.u64.u32 	%rd98, %r90;
	add.s64 	%rd99, %rd127, %rd98;
	ld.local.u32 	%r91, [%rd99];
	or.b32  	%r92, %r91, %r89;
	st.local.u32 	[%rd99], %r92;
	shl.b32 	%r93, %r87, 8;
	add.s32 	%r94, %r88, %r93;
	mul.wide.u32 	%rd100, %r94, 1028;
	add.s64 	%rd121, %rd59, %rd100;
	add.s32 	%r129, %r129, 4;
	add.s32 	%r123, %r123, -4;
	setp.ne.s32 	%p8, %r123, 0;
	@%p8 bra 	$L__BB0_4;

$L__BB0_17:
	setp.eq.s32 	%p9, %r128, 0;
	@%p9 bra 	$L__BB0_23;

	shl.b32 	%r126, %r129, 8;

$L__BB0_19:
	.pragma "nounroll";
	ld.global.u32 	%rd44, [%rd121+1024];
	and.b64  	%rd101, %rd112, -4294967296;
	setp.eq.s64 	%p10, %rd101, 0;
	@%p10 bra 	$L__BB0_21;

	div.u64 	%rd45, %rd112, %rd44;
	mul.lo.s64 	%rd102, %rd45, %rd44;
	sub.s64 	%rd126, %rd112, %rd102;
	mov.u64 	%rd112, %rd45;
	bra.uni 	$L__BB0_22;

$L__BB0_21:
	cvt.u32.u64 	%r95, %rd44;
	cvt.u32.u64 	%r96, %rd112;
	div.u32 	%r97, %r96, %r95;
	mul.lo.s32 	%r98, %r97, %r95;
	sub.s32 	%r99, %r96, %r98;
	cvt.u64.u32 	%rd112, %r97;
	cvt.u64.u32 	%rd126, %r99;

$L__BB0_22:
	shl.b32 	%r100, %r129, 3;
	not.b32 	%r101, %r100;
	and.b32  	%r102, %r101, 24;
	shl.b64 	%rd103, %rd126, 2;
	add.s64 	%rd104, %rd121, %rd103;
	ld.global.u32 	%r103, [%rd104];
	shl.b32 	%r104, %r103, %r102;
	and.b32  	%r105, %r129, -4;
	cvt.u64.u32 	%rd105, %r105;
	add.s64 	%rd106, %rd127, %rd105;
	ld.local.u32 	%r106, [%rd106];
	or.b32  	%r107, %r106, %r104;
	st.local.u32 	[%rd106], %r107;
	add.s32 	%r108, %r126, %r103;
	mul.wide.u32 	%rd107, %r108, 1028;
	add.s64 	%rd121, %rd59, %rd107;
	add.s32 	%r129, %r129, 1;
	add.s32 	%r126, %r126, 256;
	add.s32 	%r128, %r128, -1;
	setp.ne.s32 	%p11, %r128, 0;
	@%p11 bra 	$L__BB0_19;

$L__BB0_23:
	shl.b32 	%r109, %r129, 3;
	not.b32 	%r110, %r109;
	and.b32  	%r111, %r110, 24;
	mov.u32 	%r112, 255;
	shl.b32 	%r113, %r112, %r111;
	and.b32  	%r114, %r113, %r26;
	and.b32  	%r115, %r129, -4;
	cvt.u64.u32 	%rd108, %r115;
	add.s64 	%rd109, %rd127, %rd108;
	ld.local.u32 	%r116, [%rd109];
	or.b32  	%r117, %r116, %r114;
	st.local.u32 	[%rd109], %r117;
	setp.eq.s32 	%p12, %r27, 0;
	@%p12 bra 	$L__BB0_25;

	shl.b32 	%r118, %r1, 3;
	st.local.u32 	[%rd127+56], %r118;

$L__BB0_25:
	setp.eq.s32 	%p13, %r28, 0;
	@%p13 bra 	$L__BB0_27;

	shl.b32 	%r119, %r1, 3;
	st.local.u32 	[%rd127+60], %r119;

$L__BB0_27:
	mul.lo.s64 	%rd110, %rd2, 260;
	add.s64 	%rd128, %rd57, %rd110;
	mov.u32 	%r130, 0;

$L__BB0_28:
	ld.local.u32 	%r121, [%rd127];
	st.global.u32 	[%rd128], %r121;
	add.s64 	%rd128, %rd128, 4;
	add.s64 	%rd127, %rd127, 4;
	add.s32 	%r130, %r130, 1;
	setp.lt.u32 	%p14, %r130, 65;
	@%p14 bra 	$L__BB0_28;

$L__BB0_30:
	ret;

}
	// .globl	r_markov
.entry r_markov(
	.param .u64 .ptr .global .align 4 r_markov_param_0,
	.param .u64 .ptr .global .align 4 r_markov_param_1,
	.param .u64 .ptr .global .align 4 r_markov_param_2,
	.param .u64 r_markov_param_3,
	.param .u32 r_markov_param_4,
	.param .u32 r_markov_param_5,
	.param .u32 r_markov_param_6,
	.param .u32 r_markov_param_7,
	.param .u64 r_markov_param_8
)
{
	.local .align 4 .b8 	__local_depot1[260];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<12>;
	.reg .b32 	%r<99>;
	.reg .b64 	%rd<120>;


	mov.u64 	%SPL, __local_depot1;
	ld.param.u64 	%rd50, [r_markov_param_0];
	ld.param.u64 	%rd114, [r_markov_param_1];
	ld.param.u64 	%rd52, [r_markov_param_2];
	ld.param.u64 	%rd53, [r_markov_param_3];
	ld.param.u32 	%r17, [r_markov_param_4];
	ld.param.u64 	%rd54, [r_markov_param_8];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r18, %ctaid.x;
	mov.u32 	%r19, %ntid.x;
	mov.u32 	%r20, %tid.x;
	mov.b32 	%r21, %envreg3;
	add.s32 	%r22, %r20, %r21;
	mad.lo.s32 	%r23, %r19, %r18, %r22;
	cvt.s64.s32 	%rd2, %r23;
	setp.ge.u64 	%p1, %rd2, %rd54;
	@%p1 bra 	$L__BB1_24;

	mov.u32 	%r95, 0;
	st.local.u32 	[%rd1], %r95;
	st.local.u32 	[%rd1+4], %r95;
	st.local.u32 	[%rd1+8], %r95;
	st.local.u32 	[%rd1+12], %r95;
	st.local.u32 	[%rd1+16], %r95;
	st.local.u32 	[%rd1+20], %r95;
	st.local.u32 	[%rd1+24], %r95;
	st.local.u32 	[%rd1+28], %r95;
	st.local.u32 	[%rd1+32], %r95;
	st.local.u32 	[%rd1+36], %r95;
	st.local.u32 	[%rd1+40], %r95;
	st.local.u32 	[%rd1+44], %r95;
	st.local.u32 	[%rd1+48], %r95;
	st.local.u32 	[%rd1+52], %r95;
	st.local.u32 	[%rd1+56], %r95;
	st.local.u32 	[%rd1+60], %r95;
	st.local.u32 	[%rd1+64], %r95;
	st.local.u32 	[%rd1+68], %r95;
	st.local.u32 	[%rd1+72], %r95;
	st.local.u32 	[%rd1+76], %r95;
	st.local.u32 	[%rd1+80], %r95;
	st.local.u32 	[%rd1+84], %r95;
	st.local.u32 	[%rd1+88], %r95;
	st.local.u32 	[%rd1+92], %r95;
	st.local.u32 	[%rd1+96], %r95;
	st.local.u32 	[%rd1+100], %r95;
	st.local.u32 	[%rd1+104], %r95;
	st.local.u32 	[%rd1+108], %r95;
	st.local.u32 	[%rd1+112], %r95;
	st.local.u32 	[%rd1+116], %r95;
	st.local.u32 	[%rd1+120], %r95;
	st.local.u32 	[%rd1+124], %r95;
	st.local.u32 	[%rd1+128], %r95;
	st.local.u32 	[%rd1+132], %r95;
	st.local.u32 	[%rd1+136], %r95;
	st.local.u32 	[%rd1+140], %r95;
	st.local.u32 	[%rd1+144], %r95;
	st.local.u32 	[%rd1+148], %r95;
	st.local.u32 	[%rd1+152], %r95;
	st.local.u32 	[%rd1+156], %r95;
	st.local.u32 	[%rd1+160], %r95;
	st.local.u32 	[%rd1+164], %r95;
	st.local.u32 	[%rd1+168], %r95;
	st.local.u32 	[%rd1+172], %r95;
	st.local.u32 	[%rd1+176], %r95;
	st.local.u32 	[%rd1+180], %r95;
	st.local.u32 	[%rd1+184], %r95;
	st.local.u32 	[%rd1+188], %r95;
	st.local.u32 	[%rd1+192], %r95;
	st.local.u32 	[%rd1+196], %r95;
	st.local.u32 	[%rd1+200], %r95;
	st.local.u32 	[%rd1+204], %r95;
	st.local.u32 	[%rd1+208], %r95;
	st.local.u32 	[%rd1+212], %r95;
	st.local.u32 	[%rd1+216], %r95;
	st.local.u32 	[%rd1+220], %r95;
	st.local.u32 	[%rd1+224], %r95;
	st.local.u32 	[%rd1+228], %r95;
	st.local.u32 	[%rd1+232], %r95;
	st.local.u32 	[%rd1+236], %r95;
	st.local.u32 	[%rd1+240], %r95;
	st.local.u32 	[%rd1+244], %r95;
	st.local.u32 	[%rd1+248], %r95;
	st.local.u32 	[%rd1+252], %r95;
	add.s64 	%rd105, %rd2, %rd53;
	setp.eq.s32 	%p2, %r17, 0;
	@%p2 bra 	$L__BB1_23;

	add.s32 	%r26, %r17, -1;
	and.b32  	%r98, %r17, 3;
	setp.lt.u32 	%p3, %r26, 3;
	@%p3 bra 	$L__BB1_17;

	sub.s32 	%r94, %r17, %r98;
	mov.u32 	%r92, 0;
	mov.u32 	%r95, %r92;

$L__BB1_4:
	ld.global.u32 	%rd6, [%rd114+1024];
	and.b64  	%rd56, %rd105, -4294967296;
	setp.eq.s64 	%p4, %rd56, 0;
	@%p4 bra 	$L__BB1_6;

	div.u64 	%rd106, %rd105, %rd6;
	mul.lo.s64 	%rd57, %rd106, %rd6;
	sub.s64 	%rd107, %rd105, %rd57;
	bra.uni 	$L__BB1_7;

$L__BB1_6:
	cvt.u32.u64 	%r29, %rd6;
	cvt.u32.u64 	%r30, %rd105;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd106, %r31;
	cvt.u64.u32 	%rd107, %r33;

$L__BB1_7:
	shl.b64 	%rd58, %rd107, 2;
	add.s64 	%rd59, %rd114, %rd58;
	ld.global.u32 	%r34, [%rd59];
	shl.b32 	%r35, %r34, 24;
	cvt.u64.u32 	%rd60, %r95;
	add.s64 	%rd61, %rd1, %rd60;
	ld.local.u32 	%r36, [%rd61];
	or.b32  	%r37, %r36, %r35;
	st.local.u32 	[%rd61], %r37;
	add.s32 	%r38, %r92, %r34;
	cvt.u64.u32 	%rd13, %r38;
	mul.wide.u32 	%rd62, %r38, 1028;
	add.s64 	%rd63, %rd52, %rd62;
	ld.global.u32 	%rd14, [%rd63+1024];
	and.b64  	%rd64, %rd106, -4294967296;
	setp.eq.s64 	%p5, %rd64, 0;
	@%p5 bra 	$L__BB1_9;

	div.u64 	%rd108, %rd106, %rd14;
	mul.lo.s64 	%rd65, %rd108, %rd14;
	sub.s64 	%rd109, %rd106, %rd65;
	bra.uni 	$L__BB1_10;

$L__BB1_9:
	cvt.u32.u64 	%r39, %rd14;
	cvt.u32.u64 	%r40, %rd106;
	div.u32 	%r41, %r40, %r39;
	mul.lo.s32 	%r42, %r41, %r39;
	sub.s32 	%r43, %r40, %r42;
	cvt.u64.u32 	%rd108, %r41;
	cvt.u64.u32 	%rd109, %r43;

$L__BB1_10:
	mul.lo.s64 	%rd66, %rd13, 1028;
	add.s64 	%rd67, %rd52, %rd66;
	shl.b64 	%rd68, %rd109, 2;
	add.s64 	%rd69, %rd67, %rd68;
	ld.global.u32 	%r44, [%rd69];
	shl.b32 	%r45, %r44, 16;
	add.s32 	%r46, %r95, 1;
	and.b32  	%r47, %r46, -4;
	cvt.u64.u32 	%rd70, %r47;
	add.s64 	%rd71, %rd1, %rd70;
	ld.local.u32 	%r48, [%rd71];
	or.b32  	%r49, %r48, %r45;
	st.local.u32 	[%rd71], %r49;
	add.s32 	%r50, %r92, %r44;
	add.s32 	%r51, %r50, 256;
	cvt.u64.u32 	%rd21, %r51;
	mul.wide.u32 	%rd72, %r51, 1028;
	add.s64 	%rd73, %rd52, %rd72;
	ld.global.u32 	%rd22, [%rd73+1024];
	and.b64  	%rd74, %rd108, -4294967296;
	setp.eq.s64 	%p6, %rd74, 0;
	@%p6 bra 	$L__BB1_12;

	div.u64 	%rd110, %rd108, %rd22;
	mul.lo.s64 	%rd75, %rd110, %rd22;
	sub.s64 	%rd111, %rd108, %rd75;
	bra.uni 	$L__BB1_13;

$L__BB1_12:
	cvt.u32.u64 	%r52, %rd22;
	cvt.u32.u64 	%r53, %rd108;
	div.u32 	%r54, %r53, %r52;
	mul.lo.s32 	%r55, %r54, %r52;
	sub.s32 	%r56, %r53, %r55;
	cvt.u64.u32 	%rd110, %r54;
	cvt.u64.u32 	%rd111, %r56;

$L__BB1_13:
	mul.lo.s64 	%rd76, %rd21, 1028;
	add.s64 	%rd77, %rd52, %rd76;
	shl.b64 	%rd78, %rd111, 2;
	add.s64 	%rd79, %rd77, %rd78;
	ld.global.u32 	%r57, [%rd79];
	shl.b32 	%r58, %r57, 8;
	add.s32 	%r59, %r95, 2;
	and.b32  	%r60, %r59, -4;
	cvt.u64.u32 	%rd80, %r60;
	add.s64 	%rd81, %rd1, %rd80;
	ld.local.u32 	%r61, [%rd81];
	or.b32  	%r62, %r61, %r58;
	st.local.u32 	[%rd81], %r62;
	add.s32 	%r63, %r92, %r57;
	add.s32 	%r64, %r63, 512;
	cvt.u64.u32 	%rd29, %r64;
	mul.wide.u32 	%rd82, %r64, 1028;
	add.s64 	%rd83, %rd52, %rd82;
	ld.global.u32 	%rd30, [%rd83+1024];
	and.b64  	%rd84, %rd110, -4294967296;
	setp.eq.s64 	%p7, %rd84, 0;
	@%p7 bra 	$L__BB1_15;

	div.u64 	%rd105, %rd110, %rd30;
	mul.lo.s64 	%rd85, %rd105, %rd30;
	sub.s64 	%rd113, %rd110, %rd85;
	bra.uni 	$L__BB1_16;

$L__BB1_15:
	cvt.u32.u64 	%r65, %rd30;
	cvt.u32.u64 	%r66, %rd110;
	div.u32 	%r67, %r66, %r65;
	mul.lo.s32 	%r68, %r67, %r65;
	sub.s32 	%r69, %r66, %r68;
	cvt.u64.u32 	%rd105, %r67;
	cvt.u64.u32 	%rd113, %r69;

$L__BB1_16:
	add.s32 	%r70, %r95, 3;
	and.b32  	%r71, %r70, -4;
	cvt.u64.u32 	%rd86, %r71;
	add.s64 	%rd87, %rd1, %rd86;
	ld.local.u32 	%r72, [%rd87];
	mul.lo.s64 	%rd88, %rd29, 1028;
	add.s64 	%rd89, %rd52, %rd88;
	shl.b64 	%rd90, %rd113, 2;
	add.s64 	%rd91, %rd89, %rd90;
	ld.global.u32 	%r73, [%rd91];
	or.b32  	%r74, %r72, %r73;
	st.local.u32 	[%rd87], %r74;
	add.s32 	%r75, %r92, %r73;
	add.s32 	%r76, %r75, 768;
	mul.wide.u32 	%rd92, %r76, 1028;
	add.s64 	%rd114, %rd52, %rd92;
	add.s32 	%r95, %r95, 4;
	add.s32 	%r92, %r92, 1024;
	add.s32 	%r94, %r94, -4;
	setp.ne.s32 	%p8, %r94, 0;
	@%p8 bra 	$L__BB1_4;

$L__BB1_17:
	setp.eq.s32 	%p9, %r98, 0;
	@%p9 bra 	$L__BB1_23;

	shl.b32 	%r96, %r95, 8;

$L__BB1_19:
	.pragma "nounroll";
	ld.global.u32 	%rd42, [%rd114+1024];
	and.b64  	%rd93, %rd105, -4294967296;
	setp.eq.s64 	%p10, %rd93, 0;
	@%p10 bra 	$L__BB1_21;

	div.u64 	%rd43, %rd105, %rd42;
	mul.lo.s64 	%rd94, %rd43, %rd42;
	sub.s64 	%rd119, %rd105, %rd94;
	mov.u64 	%rd105, %rd43;
	bra.uni 	$L__BB1_22;

$L__BB1_21:
	cvt.u32.u64 	%r77, %rd42;
	cvt.u32.u64 	%r78, %rd105;
	div.u32 	%r79, %r78, %r77;
	mul.lo.s32 	%r80, %r79, %r77;
	sub.s32 	%r81, %r78, %r80;
	cvt.u64.u32 	%rd105, %r79;
	cvt.u64.u32 	%rd119, %r81;

$L__BB1_22:
	shl.b32 	%r82, %r95, 3;
	not.b32 	%r83, %r82;
	and.b32  	%r84, %r83, 24;
	shl.b64 	%rd95, %rd119, 2;
	add.s64 	%rd96, %rd114, %rd95;
	ld.global.u32 	%r85, [%rd96];
	shl.b32 	%r86, %r85, %r84;
	and.b32  	%r87, %r95, -4;
	cvt.u64.u32 	%rd97, %r87;
	add.s64 	%rd98, %rd1, %rd97;
	ld.local.u32 	%r88, [%rd98];
	or.b32  	%r89, %r88, %r86;
	st.local.u32 	[%rd98], %r89;
	add.s32 	%r90, %r96, %r85;
	mul.wide.u32 	%rd99, %r90, 1028;
	add.s64 	%rd114, %rd52, %rd99;
	add.s32 	%r95, %r95, 1;
	add.s32 	%r96, %r96, 256;
	add.s32 	%r98, %r98, -1;
	setp.ne.s32 	%p11, %r98, 0;
	@%p11 bra 	$L__BB1_19;

$L__BB1_23:
	ld.local.u32 	%r91, [%rd1];
	shl.b64 	%rd102, %rd2, 2;
	add.s64 	%rd103, %rd50, %rd102;
	st.global.u32 	[%rd103], %r91;

$L__BB1_24:
	ret;

}
	// .globl	C_markov
.entry C_markov(
	.param .u64 .ptr .global .align 4 C_markov_param_0,
	.param .u64 .ptr .global .align 4 C_markov_param_1,
	.param .u64 .ptr .global .align 4 C_markov_param_2,
	.param .u64 C_markov_param_3,
	.param .u32 C_markov_param_4,
	.param .u32 C_markov_param_5,
	.param .u32 C_markov_param_6,
	.param .u32 C_markov_param_7,
	.param .u64 C_markov_param_8
)
{
	.local .align 4 .b8 	__local_depot2[260];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .b32 	%r<124>;
	.reg .b64 	%rd<130>;


	mov.u64 	%SPL, __local_depot2;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd55, [C_markov_param_0];
	ld.param.u64 	%rd122, [C_markov_param_1];
	ld.param.u64 	%rd57, [C_markov_param_2];
	ld.param.u64 	%rd58, [C_markov_param_3];
	ld.param.u32 	%r21, [C_markov_param_4];
	ld.param.u32 	%r22, [C_markov_param_5];
	ld.param.u32 	%r23, [C_markov_param_6];
	ld.param.u32 	%r24, [C_markov_param_7];
	ld.param.u64 	%rd59, [C_markov_param_8];
	add.u64 	%rd60, %SP, 0;
	add.u64 	%rd128, %SPL, 0;
	mov.u32 	%r25, %ctaid.x;
	mov.u32 	%r26, %ntid.x;
	mov.u32 	%r27, %tid.x;
	mov.b32 	%r28, %envreg3;
	add.s32 	%r29, %r27, %r28;
	mad.lo.s32 	%r30, %r26, %r25, %r29;
	cvt.s64.s32 	%rd2, %r30;
	setp.ge.u64 	%p1, %rd2, %rd59;
	@%p1 bra 	$L__BB2_30;

	mov.u32 	%r122, 0;
	st.local.u32 	[%rd128], %r122;
	st.local.u32 	[%rd128+4], %r122;
	st.local.u32 	[%rd128+8], %r122;
	st.local.u32 	[%rd128+12], %r122;
	st.local.u32 	[%rd128+16], %r122;
	st.local.u32 	[%rd128+20], %r122;
	st.local.u32 	[%rd128+24], %r122;
	st.local.u32 	[%rd128+28], %r122;
	st.local.u32 	[%rd128+32], %r122;
	st.local.u32 	[%rd128+36], %r122;
	st.local.u32 	[%rd128+40], %r122;
	st.local.u32 	[%rd128+44], %r122;
	st.local.u32 	[%rd128+48], %r122;
	st.local.u32 	[%rd128+52], %r122;
	st.local.u32 	[%rd128+56], %r122;
	st.local.u32 	[%rd128+60], %r122;
	st.local.u32 	[%rd128+64], %r122;
	st.local.u32 	[%rd128+68], %r122;
	st.local.u32 	[%rd128+72], %r122;
	st.local.u32 	[%rd128+76], %r122;
	st.local.u32 	[%rd128+80], %r122;
	st.local.u32 	[%rd128+84], %r122;
	st.local.u32 	[%rd128+88], %r122;
	st.local.u32 	[%rd128+92], %r122;
	st.local.u32 	[%rd128+96], %r122;
	st.local.u32 	[%rd128+100], %r122;
	st.local.u32 	[%rd128+104], %r122;
	st.local.u32 	[%rd128+108], %r122;
	st.local.u32 	[%rd128+112], %r122;
	st.local.u32 	[%rd128+116], %r122;
	st.local.u32 	[%rd128+120], %r122;
	st.local.u32 	[%rd128+124], %r122;
	st.local.u32 	[%rd128+128], %r122;
	st.local.u32 	[%rd128+132], %r122;
	st.local.u32 	[%rd128+136], %r122;
	st.local.u32 	[%rd128+140], %r122;
	st.local.u32 	[%rd128+144], %r122;
	st.local.u32 	[%rd128+148], %r122;
	st.local.u32 	[%rd128+152], %r122;
	st.local.u32 	[%rd128+156], %r122;
	st.local.u32 	[%rd128+160], %r122;
	st.local.u32 	[%rd128+164], %r122;
	st.local.u32 	[%rd128+168], %r122;
	st.local.u32 	[%rd128+172], %r122;
	st.local.u32 	[%rd128+176], %r122;
	st.local.u32 	[%rd128+180], %r122;
	st.local.u32 	[%rd128+184], %r122;
	st.local.u32 	[%rd128+188], %r122;
	st.local.u32 	[%rd128+192], %r122;
	st.local.u32 	[%rd128+196], %r122;
	st.local.u32 	[%rd128+200], %r122;
	st.local.u32 	[%rd128+204], %r122;
	st.local.u32 	[%rd128+208], %r122;
	st.local.u32 	[%rd128+212], %r122;
	st.local.u32 	[%rd128+216], %r122;
	st.local.u32 	[%rd128+220], %r122;
	st.local.u32 	[%rd128+224], %r122;
	st.local.u32 	[%rd128+228], %r122;
	st.local.u32 	[%rd128+232], %r122;
	st.local.u32 	[%rd128+236], %r122;
	st.local.u32 	[%rd128+240], %r122;
	st.local.u32 	[%rd128+244], %r122;
	st.local.u32 	[%rd128+248], %r122;
	st.local.u32 	[%rd128+252], %r122;
	st.local.u32 	[%rd128+256], %r21;
	add.s64 	%rd113, %rd2, %rd58;
	setp.eq.s32 	%p2, %r21, 0;
	@%p2 bra 	$L__BB2_23;

	add.s32 	%r34, %r21, -1;
	setp.lt.u32 	%p3, %r34, 3;
	mov.u32 	%r122, 0;
	@%p3 bra 	$L__BB2_17;

	and.b32  	%r37, %r21, 3;
	sub.s32 	%r116, %r21, %r37;
	mov.u32 	%r114, %r122;

$L__BB2_4:
	ld.global.u32 	%rd6, [%rd122+1024];
	and.b64  	%rd61, %rd113, -4294967296;
	setp.eq.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB2_6;

	div.u64 	%rd114, %rd113, %rd6;
	mul.lo.s64 	%rd62, %rd114, %rd6;
	sub.s64 	%rd115, %rd113, %rd62;
	bra.uni 	$L__BB2_7;

$L__BB2_6:
	cvt.u32.u64 	%r38, %rd6;
	cvt.u32.u64 	%r39, %rd113;
	div.u32 	%r40, %r39, %r38;
	mul.lo.s32 	%r41, %r40, %r38;
	sub.s32 	%r42, %r39, %r41;
	cvt.u64.u32 	%rd114, %r40;
	cvt.u64.u32 	%rd115, %r42;

$L__BB2_7:
	shl.b64 	%rd63, %rd115, 2;
	add.s64 	%rd64, %rd122, %rd63;
	ld.global.u32 	%r43, [%rd64];
	shl.b32 	%r44, %r43, 24;
	cvt.u64.u32 	%rd65, %r122;
	add.s64 	%rd66, %rd128, %rd65;
	ld.local.u32 	%r45, [%rd66];
	or.b32  	%r46, %r45, %r44;
	st.local.u32 	[%rd66], %r46;
	add.s32 	%r47, %r114, %r43;
	cvt.u64.u32 	%rd13, %r47;
	mul.wide.u32 	%rd67, %r47, 1028;
	add.s64 	%rd68, %rd57, %rd67;
	ld.global.u32 	%rd14, [%rd68+1024];
	and.b64  	%rd69, %rd114, -4294967296;
	setp.eq.s64 	%p5, %rd69, 0;
	@%p5 bra 	$L__BB2_9;

	div.u64 	%rd116, %rd114, %rd14;
	mul.lo.s64 	%rd70, %rd116, %rd14;
	sub.s64 	%rd117, %rd114, %rd70;
	bra.uni 	$L__BB2_10;

$L__BB2_9:
	cvt.u32.u64 	%r48, %rd14;
	cvt.u32.u64 	%r49, %rd114;
	div.u32 	%r50, %r49, %r48;
	mul.lo.s32 	%r51, %r50, %r48;
	sub.s32 	%r52, %r49, %r51;
	cvt.u64.u32 	%rd116, %r50;
	cvt.u64.u32 	%rd117, %r52;

$L__BB2_10:
	mul.lo.s64 	%rd71, %rd13, 1028;
	add.s64 	%rd72, %rd57, %rd71;
	shl.b64 	%rd73, %rd117, 2;
	add.s64 	%rd74, %rd72, %rd73;
	ld.global.u32 	%r53, [%rd74];
	shl.b32 	%r54, %r53, 16;
	add.s32 	%r55, %r122, 1;
	and.b32  	%r56, %r55, -4;
	cvt.u64.u32 	%rd75, %r56;
	add.s64 	%rd76, %rd128, %rd75;
	ld.local.u32 	%r57, [%rd76];
	or.b32  	%r58, %r57, %r54;
	st.local.u32 	[%rd76], %r58;
	add.s32 	%r59, %r114, %r53;
	add.s32 	%r60, %r59, 256;
	cvt.u64.u32 	%rd21, %r60;
	mul.wide.u32 	%rd77, %r60, 1028;
	add.s64 	%rd78, %rd57, %rd77;
	ld.global.u32 	%rd22, [%rd78+1024];
	and.b64  	%rd79, %rd116, -4294967296;
	setp.eq.s64 	%p6, %rd79, 0;
	@%p6 bra 	$L__BB2_12;

	div.u64 	%rd118, %rd116, %rd22;
	mul.lo.s64 	%rd80, %rd118, %rd22;
	sub.s64 	%rd119, %rd116, %rd80;
	bra.uni 	$L__BB2_13;

$L__BB2_12:
	cvt.u32.u64 	%r61, %rd22;
	cvt.u32.u64 	%r62, %rd116;
	div.u32 	%r63, %r62, %r61;
	mul.lo.s32 	%r64, %r63, %r61;
	sub.s32 	%r65, %r62, %r64;
	cvt.u64.u32 	%rd118, %r63;
	cvt.u64.u32 	%rd119, %r65;

$L__BB2_13:
	mul.lo.s64 	%rd81, %rd21, 1028;
	add.s64 	%rd82, %rd57, %rd81;
	shl.b64 	%rd83, %rd119, 2;
	add.s64 	%rd84, %rd82, %rd83;
	ld.global.u32 	%r66, [%rd84];
	shl.b32 	%r67, %r66, 8;
	add.s32 	%r68, %r122, 2;
	and.b32  	%r69, %r68, -4;
	cvt.u64.u32 	%rd85, %r69;
	add.s64 	%rd86, %rd128, %rd85;
	ld.local.u32 	%r70, [%rd86];
	or.b32  	%r71, %r70, %r67;
	st.local.u32 	[%rd86], %r71;
	add.s32 	%r72, %r114, %r66;
	add.s32 	%r73, %r72, 512;
	cvt.u64.u32 	%rd29, %r73;
	mul.wide.u32 	%rd87, %r73, 1028;
	add.s64 	%rd88, %rd57, %rd87;
	ld.global.u32 	%rd30, [%rd88+1024];
	and.b64  	%rd89, %rd118, -4294967296;
	setp.eq.s64 	%p7, %rd89, 0;
	@%p7 bra 	$L__BB2_15;

	div.u64 	%rd113, %rd118, %rd30;
	mul.lo.s64 	%rd90, %rd113, %rd30;
	sub.s64 	%rd121, %rd118, %rd90;
	bra.uni 	$L__BB2_16;

$L__BB2_15:
	cvt.u32.u64 	%r74, %rd30;
	cvt.u32.u64 	%r75, %rd118;
	div.u32 	%r76, %r75, %r74;
	mul.lo.s32 	%r77, %r76, %r74;
	sub.s32 	%r78, %r75, %r77;
	cvt.u64.u32 	%rd113, %r76;
	cvt.u64.u32 	%rd121, %r78;

$L__BB2_16:
	add.s32 	%r79, %r122, 3;
	and.b32  	%r80, %r79, -4;
	cvt.u64.u32 	%rd91, %r80;
	add.s64 	%rd92, %rd128, %rd91;
	ld.local.u32 	%r81, [%rd92];
	mul.lo.s64 	%rd93, %rd29, 1028;
	add.s64 	%rd94, %rd57, %rd93;
	shl.b64 	%rd95, %rd121, 2;
	add.s64 	%rd96, %rd94, %rd95;
	ld.global.u32 	%r82, [%rd96];
	or.b32  	%r83, %r81, %r82;
	st.local.u32 	[%rd92], %r83;
	add.s32 	%r84, %r114, %r82;
	add.s32 	%r85, %r84, 768;
	mul.wide.u32 	%rd97, %r85, 1028;
	add.s64 	%rd122, %rd57, %rd97;
	add.s32 	%r122, %r122, 4;
	add.s32 	%r114, %r114, 1024;
	add.s32 	%r116, %r116, -4;
	setp.ne.s32 	%p8, %r116, 0;
	@%p8 bra 	$L__BB2_4;

$L__BB2_17:
	and.b32  	%r121, %r21, 3;
	setp.eq.s32 	%p9, %r121, 0;
	@%p9 bra 	$L__BB2_23;

	shl.b32 	%r119, %r122, 8;

$L__BB2_19:
	.pragma "nounroll";
	ld.global.u32 	%rd42, [%rd122+1024];
	and.b64  	%rd98, %rd113, -4294967296;
	setp.eq.s64 	%p10, %rd98, 0;
	@%p10 bra 	$L__BB2_21;

	div.u64 	%rd43, %rd113, %rd42;
	mul.lo.s64 	%rd99, %rd43, %rd42;
	sub.s64 	%rd127, %rd113, %rd99;
	mov.u64 	%rd113, %rd43;
	bra.uni 	$L__BB2_22;

$L__BB2_21:
	cvt.u32.u64 	%r87, %rd42;
	cvt.u32.u64 	%r88, %rd113;
	div.u32 	%r89, %r88, %r87;
	mul.lo.s32 	%r90, %r89, %r87;
	sub.s32 	%r91, %r88, %r90;
	cvt.u64.u32 	%rd113, %r89;
	cvt.u64.u32 	%rd127, %r91;

$L__BB2_22:
	shl.b32 	%r92, %r122, 3;
	not.b32 	%r93, %r92;
	and.b32  	%r94, %r93, 24;
	shl.b64 	%rd100, %rd127, 2;
	add.s64 	%rd101, %rd122, %rd100;
	ld.global.u32 	%r95, [%rd101];
	shl.b32 	%r96, %r95, %r94;
	and.b32  	%r97, %r122, -4;
	cvt.u64.u32 	%rd102, %r97;
	add.s64 	%rd103, %rd128, %rd102;
	ld.local.u32 	%r98, [%rd103];
	or.b32  	%r99, %r98, %r96;
	st.local.u32 	[%rd103], %r99;
	add.s32 	%r100, %r119, %r95;
	mul.wide.u32 	%rd104, %r100, 1028;
	add.s64 	%rd122, %rd57, %rd104;
	add.s32 	%r122, %r122, 1;
	add.s32 	%r119, %r119, 256;
	add.s32 	%r121, %r121, -1;
	setp.ne.s32 	%p11, %r121, 0;
	@%p11 bra 	$L__BB2_19;

$L__BB2_23:
	shl.b32 	%r101, %r122, 3;
	not.b32 	%r102, %r101;
	and.b32  	%r103, %r102, 24;
	mov.u32 	%r104, 255;
	shl.b32 	%r105, %r104, %r103;
	and.b32  	%r106, %r105, %r22;
	and.b32  	%r107, %r122, -4;
	cvt.u64.u32 	%rd105, %r107;
	add.s64 	%rd106, %rd128, %rd105;
	ld.local.u32 	%r108, [%rd106];
	or.b32  	%r109, %r108, %r106;
	st.local.u32 	[%rd106], %r109;
	setp.eq.s32 	%p12, %r23, 0;
	@%p12 bra 	$L__BB2_25;

	shl.b32 	%r110, %r21, 3;
	cvta.to.local.u64 	%rd108, %rd60;
	st.local.u32 	[%rd108+56], %r110;

$L__BB2_25:
	setp.eq.s32 	%p13, %r24, 0;
	@%p13 bra 	$L__BB2_27;

	shl.b32 	%r111, %r21, 3;
	cvta.to.local.u64 	%rd110, %rd60;
	st.local.u32 	[%rd110+60], %r111;

$L__BB2_27:
	mul.lo.s64 	%rd111, %rd2, 260;
	add.s64 	%rd129, %rd55, %rd111;
	mov.u32 	%r123, 0;

$L__BB2_28:
	ld.local.u32 	%r113, [%rd128];
	st.global.u32 	[%rd129], %r113;
	add.s64 	%rd129, %rd129, 4;
	add.s64 	%rd128, %rd128, 4;
	add.s32 	%r123, %r123, 1;
	setp.lt.u32 	%p14, %r123, 65;
	@%p14 bra 	$L__BB2_28;

$L__BB2_30:
	ret;

}

  